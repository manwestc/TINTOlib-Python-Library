{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Third-party libraries\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Scikit-learn - core modules\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Scikit-learn - metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, average_precision_score, balanced_accuracy_score,\n",
    "    ConfusionMatrixDisplay, f1_score, log_loss,\n",
    "    matthews_corrcoef, mean_squared_error, precision_score,\n",
    "    PrecisionRecallDisplay, r2_score, recall_score, roc_auc_score, RocCurveDisplay\n",
    ")\n",
    "\n",
    "# Local application/library imports\n",
    "from utils import load_search_space, get_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 64\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "# adult_income_cleaned, framingham_cleaned, preprocessed_heloc, diabetes\n",
    "dataset_name = 'mfeat-fourier'        \n",
    "dataset_subpath = 'Multiclass/mfeat-fourier'       \n",
    "task_type = 'Multiclass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "# adult_income_cleaned, framingham_cleaned, preprocessed_heloc, diabetes\n",
    "dataset_name = 'boston'        \n",
    "dataset_subpath = 'Regression/boston'       \n",
    "task_type = 'Regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"./data/{dataset_subpath}/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LOAD AND PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, dataset_name, task_type, model_type=\"default\", seed=42):\n",
    "    task_type = task_type.lower()\n",
    "    model_type = model_type.lower()\n",
    "\n",
    "    # Load config\n",
    "    with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    categorical_cols = config[\"categorical_cols\"]\n",
    "    numerical_cols = config[\"numerical_cols\"]\n",
    "    encoding = config[\"encoding\"]\n",
    "\n",
    "    # Extract features and target\n",
    "    X = df[numerical_cols + categorical_cols].copy()\n",
    "    y = df.iloc[:, -1].copy()\n",
    "\n",
    "    # Encode target if needed\n",
    "    le = None\n",
    "    if encoding.get(\"target\") == \"label\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "        label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    else:\n",
    "        label_mapping = None\n",
    "\n",
    "    # Split raw data before transformation\n",
    "    if task_type == \"regression\":\n",
    "        # For regression, we can use a simple split\n",
    "        X_train_raw, X_temp_raw, y_train, y_temp = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=seed\n",
    "        )\n",
    "        X_val_raw, X_test_raw, y_val, y_test = train_test_split(\n",
    "            X_temp_raw, y_temp, test_size=0.5, random_state=seed\n",
    "        )\n",
    "    else:\n",
    "        # For classification, we need stratified splits\n",
    "        X_train_raw, X_temp_raw, y_train, y_temp = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=seed, stratify=y\n",
    "        )\n",
    "        X_val_raw, X_test_raw, y_val, y_test = train_test_split(\n",
    "            X_temp_raw, y_temp, test_size=0.5, random_state=seed, stratify=y_temp\n",
    "        )\n",
    "\n",
    "    # Ensure y_* are Series with index matching the X_*\n",
    "    y_train = pd.Series(y_train, index=X_train_raw.index)\n",
    "    y_val = pd.Series(y_val, index=X_val_raw.index)\n",
    "    y_test = pd.Series(y_test, index=X_test_raw.index)\n",
    "\n",
    "    # Compute class weights for classification\n",
    "    class_weight = None\n",
    "    if task_type in [\"binary\", \"multiclass\"]:\n",
    "        class_weight_values = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "        # Create the class weight dictionary with keys as native Python int (not numpy.int32)\n",
    "        class_weight = dict(zip([int(key) for key in np.unique(y_train)], class_weight_values))\n",
    "        print(f\"Class weights: {class_weight}\")\n",
    "\n",
    "    # CATBOOST path (no transformation, native categorical handling)\n",
    "    if model_type == \"catboost\":\n",
    "        for col in categorical_cols:\n",
    "            X_train_raw[col] = X_train_raw[col].astype(str)\n",
    "            X_val_raw[col] = X_val_raw[col].astype(str)\n",
    "            X_test_raw[col] = X_test_raw[col].astype(str)\n",
    "        print(f\"Shapes — Train: {X_train_raw.shape}, Val: {X_val_raw.shape}, Test: {X_test_raw.shape}\")\n",
    "        print(f\"Numerical features: {len(numerical_cols)} — {numerical_cols}\")\n",
    "        print(f\"Categorical features: {len(categorical_cols)} — {categorical_cols}\")\n",
    "        print(f\"Total features: {X_train_raw.shape[1]}\")\n",
    "        if label_mapping:\n",
    "            print(f\"Target label mapping: {label_mapping}\")\n",
    "        return (\n",
    "            X_train_raw, X_val_raw, X_test_raw,\n",
    "            y_train, y_val, y_test,\n",
    "            categorical_cols, le, class_weight\n",
    "        )\n",
    "\n",
    "    # Transform numerical and categorical features\n",
    "    transformers = []\n",
    "\n",
    "    if encoding[\"numerical_features\"] == \"minmax\":\n",
    "        transformers.append((\"num\", MinMaxScaler(), numerical_cols))\n",
    "    elif encoding[\"numerical_features\"] == \"standard\":\n",
    "        transformers.append((\"num\", StandardScaler(), numerical_cols))\n",
    "\n",
    "    if categorical_cols and encoding[\"categorical_features\"] == \"onehot\":\n",
    "        transformers.append((\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), categorical_cols))\n",
    "\n",
    "    if transformers:\n",
    "        preprocessor = ColumnTransformer(transformers=transformers)\n",
    "        X_train = preprocessor.fit_transform(X_train_raw)\n",
    "        X_val = preprocessor.transform(X_val_raw)\n",
    "        X_test = preprocessor.transform(X_test_raw)\n",
    "\n",
    "        # Recover transformed column names\n",
    "        if \"cat\" in preprocessor.named_transformers_:\n",
    "            cat_feature_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_cols)\n",
    "            all_feature_names = numerical_cols + list(cat_feature_names)\n",
    "        else:\n",
    "            all_feature_names = numerical_cols + categorical_cols\n",
    "\n",
    "        X_train = pd.DataFrame(X_train, columns=all_feature_names, index=X_train_raw.index)\n",
    "        X_val = pd.DataFrame(X_val, columns=all_feature_names, index=X_val_raw.index)\n",
    "        X_test = pd.DataFrame(X_test, columns=all_feature_names, index=X_test_raw.index)\n",
    "    else:\n",
    "        all_feature_names = numerical_cols + categorical_cols  # or keep original order\n",
    "        X_train = pd.DataFrame(X_train_raw, columns=all_feature_names, index=X_train_raw.index)\n",
    "        X_val = pd.DataFrame(X_val_raw, columns=all_feature_names, index=X_val_raw.index)\n",
    "        X_test = pd.DataFrame(X_test_raw, columns=all_feature_names, index=X_test_raw.index)\n",
    "\n",
    "    print(f\"Shapes — Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    print(f\"Numerical features: {len(numerical_cols)} — {numerical_cols}\")\n",
    "    print(f\"Categorical features: {len(categorical_cols)} — {categorical_cols}\")\n",
    "    print(f\"Total features: {X_train.shape[1]}\")\n",
    "    if label_mapping:\n",
    "        print(f\"Target label mapping: {label_mapping}\")\n",
    "\n",
    "    return (\n",
    "        X_train, X_val, X_test,\n",
    "        y_train, y_val, y_test,\n",
    "        None, le, class_weight\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## COMPILE AND FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, name, task_type, \n",
    "              X_train, y_train, \n",
    "              X_val, y_val, \n",
    "              metric_name, categorical_cols = None, num_classes=None, SEED=42, device='cuda', save_dir=None, class_weight=None):\n",
    "    params = load_search_space(name, trial)\n",
    "\n",
    "    if name == \"catboost\":\n",
    "        params[\"cat_features\"] = categorical_cols\n",
    "\n",
    "    model = get_model(name, params, task_type, num_classes, SEED, device, class_weight=class_weight)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Choose metric based on task\n",
    "    metric_name = metric_name.lower()\n",
    "    if metric_name == \"f1\":\n",
    "        score = f1_score(y_val, y_pred, average='macro')  # F1 Score\n",
    "        metric_name = \"F1\"\n",
    "\n",
    "    elif metric_name == \"accuracy\":\n",
    "        score = accuracy_score(y_val, y_pred)  # Accuracy\n",
    "        metric_name = \"Accuracy\"\n",
    "\n",
    "    elif metric_name == \"mse\":\n",
    "        score = mean_squared_error(y_val, y_pred)  # MSE\n",
    "        metric_name = \"MSE\"\n",
    "\n",
    "    elif metric_name == \"rmse\":\n",
    "        score = np.sqrt(mean_squared_error(y_val, y_pred))  # RMSE\n",
    "        metric_name = \"RMSE\"\n",
    "\n",
    "    elif metric_name == \"auc\":\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_proba = model.predict_proba(X_val)\n",
    "            if y_proba.shape[1] == 2:\n",
    "                # Binary classification\n",
    "                y_score = y_proba[:, 1]\n",
    "                score = roc_auc_score(y_val, y_score)\n",
    "            else:\n",
    "                # Multiclass\n",
    "                score = roc_auc_score(y_val, y_proba, multi_class=\"ovr\", average=\"macro\")\n",
    "        else:\n",
    "            raise ValueError(\"Model does not support predict_proba, required for AUC.\")\n",
    "        metric_name = \"AUC\"\n",
    "\n",
    "    save_dir = os.path.join(save_dir, name, \"optuna\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "        f.write(f\"Trial {trial.number} - VAL-{metric_name}: {score:.4f}, Params: {params}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_best_model(study, name, task_type,\n",
    "                        X_train, y_train,\n",
    "                        X_val, y_val,\n",
    "                        X_test, y_test,\n",
    "                        categorical_cols=None, num_classes=None, SEED=42, device='cuda', save_dir=None, class_weight=None):\n",
    "\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    if name == \"catboost\":\n",
    "        best_params[\"cat_features\"] = categorical_cols\n",
    "        \n",
    "    model = get_model(name, best_params, task_type, num_classes, SEED, device, class_weight=class_weight)\n",
    "\n",
    "    save_path = os.path.join(save_dir, name, \"best_model\")\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Train model\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_train\n",
    "\n",
    "    log = {\n",
    "        \"Training Time (s)\": train_time,\n",
    "    }\n",
    "\n",
    "    task_type = task_type.lower()\n",
    "\n",
    "    def evaluate_split(X, y, split_name):\n",
    "        start_pred = time.time()\n",
    "        y_pred = model.predict(X)\n",
    "        pred_time = time.time() - start_pred\n",
    "\n",
    "        y_prob = model.predict_proba(X)[:, 1] if hasattr(model, \"predict_proba\") and task_type == \"binary\" else None\n",
    "        result = {}\n",
    "        result[\"Inference Time (s)\"] = pred_time\n",
    "        result[\"Accuracy\"] = accuracy_score(y, y_pred)\n",
    "        result[\"F1\"] = f1_score(y, y_pred, average=\"macro\")\n",
    "        result[\"Recall\"] = recall_score(y, y_pred, average=\"macro\")\n",
    "        result[\"Precision\"] = precision_score(y, y_pred, average=\"macro\" if task_type == \"multiclass\" else \"binary\")\n",
    "        result[\"Balanced Accuracy\"] = balanced_accuracy_score(y, y_pred)\n",
    "\n",
    "        if task_type == \"binary\":\n",
    "            result[\"MCC\"] = matthews_corrcoef(y, y_pred)\n",
    "        \n",
    "        if task_type == \"binary\" and y_prob is not None:\n",
    "            result[\"AUC\"] = roc_auc_score(y, y_prob)\n",
    "            result[\"Avg Precision\"] = average_precision_score(y, y_prob)\n",
    "            result[\"Log Loss\"] = log_loss(y, y_prob)\n",
    "\n",
    "        # Print results directly\n",
    "        print(f\"\\n--- {split_name} Results ---\")\n",
    "        for k, v in result.items():\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "        return {f\"{split_name} {k}\": v for k, v in result.items()}\n",
    "\n",
    "    if task_type == \"regression\":\n",
    "        def evaluate_regression(X, y, split_name):\n",
    "            y_pred = model.predict(X)\n",
    "            result = {\n",
    "                f\"{split_name} MSE\": mean_squared_error(y, y_pred),\n",
    "                f\"{split_name} RMSE\": np.sqrt(mean_squared_error(y, y_pred)),\n",
    "                f\"{split_name} R2\": r2_score(y, y_pred)\n",
    "            }\n",
    "            print(f\"\\n--- {split_name} Regression Results ---\")\n",
    "            for k, v in result.items():\n",
    "                print(f\"{k}: {v:.4f}\")\n",
    "            return result\n",
    "\n",
    "        log.update(evaluate_regression(X_train, y_train, \"Train\"))\n",
    "        log.update(evaluate_regression(X_val, y_val, \"Val\"))\n",
    "        log.update(evaluate_regression(X_test, y_test, \"Test\"))\n",
    "\n",
    "    elif task_type in [\"binary\", \"multiclass\"]:\n",
    "        log.update(evaluate_split(X_train, y_train, \"Train\"))\n",
    "        log.update(evaluate_split(X_val, y_val, \"Val\"))\n",
    "        log.update(evaluate_split(X_test, y_test, \"Test\"))\n",
    "\n",
    "    # Save model\n",
    "    model_file = os.path.join(save_path, \"best_model.joblib\")\n",
    "    joblib.dump(model, model_file)\n",
    "\n",
    "    # Save metrics\n",
    "    log_file = os.path.join(save_path, \"best_model_metrics.txt\")\n",
    "    with open(log_file, \"w\") as f:\n",
    "        for key, value in log.items():\n",
    "            f.write(f\"{key}: {value:.4f}\\n\")\n",
    "\n",
    "    print(\"\\nBest Parameters:\")\n",
    "    print(best_params)\n",
    "\n",
    "    # Save best hyperparameters\n",
    "    params_file = os.path.join(save_path, \"best_params.json\")\n",
    "    with open(params_file, \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "\n",
    "    print(f\"\\nModel saved: {model_file}\")\n",
    "    print(f\"Metrics saved: {log_file}\")\n",
    "    return log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_model_seed(seed: int):\n",
    "    # Python built-in RNG\n",
    "    random.seed(seed)\n",
    "    # NumPy RNG\n",
    "    np.random.seed(seed)\n",
    "    # Torch RNG\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you use multi-GPU\n",
    "    \n",
    "    # For reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "save_dir =  os.path.join(\"logs\", task_type, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metric and direction based on task_type\n",
    "if task_type.lower() == 'regression':\n",
    "    metric_name = \"RMSE\"  # or any other regression metric\n",
    "    direction = \"minimize\"  # Lower RMSE is better\n",
    "elif task_type.lower() == 'binary':\n",
    "    metric_name = \"AUC\"  # or any other binary classification metric\n",
    "    direction = \"maximize\"  # Higher AUC is better\n",
    "elif task_type.lower() == 'multiclass':\n",
    "    metric_name = \"Accuracy\"  # or any other multiclass classification metric\n",
    "    direction = \"maximize\"  # Higher accuracy is better\n",
    "else:\n",
    "    raise ValueError(f\"Unknown task_type: {task_type}\")\n",
    "\n",
    "print(metric_name, direction)\n",
    "\n",
    "if task_type.lower() == 'regression' or task_type.lower() == 'binary':\n",
    "    num_classes = None\n",
    "else:\n",
    "    num_classes = df.iloc[:, -1].nunique()\n",
    "    print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, categorical_cols, label_encoder, class_weight = preprocess_data(df, dataset_name=dataset_name, task_type=task_type, model_type=\"xgboost\", seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=direction)\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    name=\"xgboost\",\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    metric_name=metric_name,\n",
    "    SEED=SEED,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=class_weight,\n",
    "), n_trials=100)\n",
    "\n",
    "# Print best result summary\n",
    "best_trial = study.best_trial\n",
    "print(f\"\\nBest Trial: {best_trial.number}\")\n",
    "print(f\"  Score: {best_trial.value:.4f}\")\n",
    "print(\"  Best Hyperparameters:\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"    {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "\n",
    "# seeds & aggregation\n",
    "model_seeds = [0, 1, 2, 3, 4]\n",
    "numeric_keys = None\n",
    "per_seed_metrics = []\n",
    "\n",
    "# Where to save the summary file (the same folder you used before)\n",
    "summary_dir = os.path.join(save_dir, f\"xgboost/best_model\")\n",
    "os.makedirs(summary_dir, exist_ok=True)\n",
    "out_file = os.path.join(summary_dir, \"best_results_mean.txt\")\n",
    "\n",
    "for s in model_seeds:\n",
    "    set_model_seed(s)  # your util to set np/torch/python seeds if needed\n",
    "    metrics = evaluate_best_model(\n",
    "        study,                 # Optuna study with .best_params\n",
    "        \"xgboost\",                  # \"xgboost\" / \"lightgbm\" / \"catboost\" / etc.\n",
    "        task_type,\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        X_test, y_test,\n",
    "        categorical_cols=categorical_cols,\n",
    "        num_classes=num_classes,\n",
    "        SEED=s,                # <<< pass the seed into the model\n",
    "        device=device,\n",
    "        save_dir=save_dir,\n",
    "        class_weight=class_weight\n",
    "    )\n",
    "    if not isinstance(metrics, dict):\n",
    "        raise TypeError(f\"evaluate_best_model must return dict, got {type(metrics)}\")\n",
    "\n",
    "    if numeric_keys is None:\n",
    "        numeric_keys = [k for k, v in metrics.items() if isinstance(v, (Number, np.floating, np.integer))]\n",
    "    per_seed_metrics.append(metrics)\n",
    "\n",
    "    # brief print\n",
    "    brief = \", \".join(f\"{k}={float(metrics[k]):.6f}\" for k in numeric_keys[:6])\n",
    "    print(f\"Seed {s}: {brief}\")\n",
    "\n",
    "# Aggregate mean/std\n",
    "aggregates = {}\n",
    "for k in numeric_keys:\n",
    "    vals = [float(m[k]) for m in per_seed_metrics]\n",
    "    mean_k = float(np.mean(vals))\n",
    "    std_k  = float(np.std(vals, ddof=1)) if len(vals) > 1 else 0.0\n",
    "    aggregates[k] = {\"mean\": mean_k, \"std\": std_k}\n",
    "\n",
    "# Save YAML-like txt (same style as your example)\n",
    "with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Best trial re-evaluation across model seeds\\n\")\n",
    "    # If you really want to include patch_size (only exists for ViT/CNN), guard it:\n",
    "    if hasattr(study, \"best_params\") and \"patch_size\" in study.best_params:\n",
    "        f.write(f\"patch_size: {study.best_params['patch_size']}\\n\")\n",
    "    f.write(f\"seeds: {model_seeds}\\n\")\n",
    "    f.write(\"per_seed_metrics:\\n\")\n",
    "    for s, m in zip(model_seeds, per_seed_metrics):\n",
    "        f.write(f\"  - seed: {s}\\n\")\n",
    "        for k in numeric_keys:\n",
    "            f.write(f\"      {k}: {float(m[k]):.6f}\\n\")\n",
    "    f.write(\"aggregates:\\n\")\n",
    "    for k, mm in aggregates.items():\n",
    "        f.write(f\"  {k}:\\n\")\n",
    "        f.write(f\"    mean: {mm['mean']:.6f}\\n\")\n",
    "        f.write(f\"    std: {mm['std']:.6f}\\n\")\n",
    "\n",
    "# Console summary (pick a sensible key)\n",
    "pref_key = None\n",
    "if task_type.lower() == \"binary\":\n",
    "    for cand in [\"Test AUC\", \"Test Accuracy\", \"Val AUC\", \"Val Accuracy\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "elif task_type.lower() == \"multiclass\":\n",
    "    for cand in [\"Test Accuracy\", \"Val Accuracy\", \"Test F1\", \"Val F1\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "else:  # regression\n",
    "    for cand in [\"Test RMSE\", \"Val RMSE\", \"Test R2\", \"Val R2\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "\n",
    "if pref_key:\n",
    "    print(f\"→ xgboost: {pref_key} = {aggregates[pref_key]['mean']:.6f} ± {aggregates[pref_key]['std']:.6f}\")\n",
    "print(f\"Saved to: {out_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, categorical_cols, label_encoder, class_weight = preprocess_data(df, dataset_name=dataset_name, task_type=task_type, model_type=\"catboost\", seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "categorical_cols = config[\"categorical_cols\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=direction)\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    name=\"catboost\",\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    metric_name=metric_name,\n",
    "    SEED=SEED,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=class_weight,\n",
    "    categorical_cols=categorical_cols\n",
    "), n_trials=50)\n",
    "\n",
    "# Print best result summary\n",
    "best_trial = study.best_trial\n",
    "print(f\"\\nBest Trial: {best_trial.number}\")\n",
    "print(f\"  AUC Score: {best_trial.value:.4f}\")\n",
    "print(\"  Best Hyperparameters:\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"    {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "\n",
    "# seeds & aggregation\n",
    "model_seeds = [0, 1, 2, 3, 4]\n",
    "numeric_keys = None\n",
    "per_seed_metrics = []\n",
    "\n",
    "# Where to save the summary file (the same folder you used before)\n",
    "summary_dir = os.path.join(save_dir, f\"catboost/best_model\")\n",
    "os.makedirs(summary_dir, exist_ok=True)\n",
    "out_file = os.path.join(summary_dir, \"best_results_mean.txt\")\n",
    "\n",
    "for s in model_seeds:\n",
    "    set_model_seed(s)  # your util to set np/torch/python seeds if needed\n",
    "    metrics = evaluate_best_model(\n",
    "        study,                 # Optuna study with .best_params\n",
    "        \"catboost\",                  # \"xgboost\" / \"lightgbm\" / \"catboost\" / etc.\n",
    "        task_type,\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        X_test, y_test,\n",
    "        categorical_cols=categorical_cols,\n",
    "        num_classes=num_classes,\n",
    "        SEED=s,                # <<< pass the seed into the model\n",
    "        device=device,\n",
    "        save_dir=save_dir,\n",
    "        class_weight=class_weight\n",
    "    )\n",
    "    if not isinstance(metrics, dict):\n",
    "        raise TypeError(f\"evaluate_best_model must return dict, got {type(metrics)}\")\n",
    "\n",
    "    if numeric_keys is None:\n",
    "        numeric_keys = [k for k, v in metrics.items() if isinstance(v, (Number, np.floating, np.integer))]\n",
    "    per_seed_metrics.append(metrics)\n",
    "\n",
    "    # brief print\n",
    "    brief = \", \".join(f\"{k}={float(metrics[k]):.6f}\" for k in numeric_keys[:6])\n",
    "    print(f\"Seed {s}: {brief}\")\n",
    "\n",
    "# Aggregate mean/std\n",
    "aggregates = {}\n",
    "for k in numeric_keys:\n",
    "    vals = [float(m[k]) for m in per_seed_metrics]\n",
    "    mean_k = float(np.mean(vals))\n",
    "    std_k  = float(np.std(vals, ddof=1)) if len(vals) > 1 else 0.0\n",
    "    aggregates[k] = {\"mean\": mean_k, \"std\": std_k}\n",
    "\n",
    "# Save YAML-like txt (same style as your example)\n",
    "with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Best trial re-evaluation across model seeds\\n\")\n",
    "    # If you really want to include patch_size (only exists for ViT/CNN), guard it:\n",
    "    if hasattr(study, \"best_params\") and \"patch_size\" in study.best_params:\n",
    "        f.write(f\"patch_size: {study.best_params['patch_size']}\\n\")\n",
    "    f.write(f\"seeds: {model_seeds}\\n\")\n",
    "    f.write(\"per_seed_metrics:\\n\")\n",
    "    for s, m in zip(model_seeds, per_seed_metrics):\n",
    "        f.write(f\"  - seed: {s}\\n\")\n",
    "        for k in numeric_keys:\n",
    "            f.write(f\"      {k}: {float(m[k]):.6f}\\n\")\n",
    "    f.write(\"aggregates:\\n\")\n",
    "    for k, mm in aggregates.items():\n",
    "        f.write(f\"  {k}:\\n\")\n",
    "        f.write(f\"    mean: {mm['mean']:.6f}\\n\")\n",
    "        f.write(f\"    std: {mm['std']:.6f}\\n\")\n",
    "\n",
    "# Console summary (pick a sensible key)\n",
    "pref_key = None\n",
    "if task_type.lower() == \"binary\":\n",
    "    for cand in [\"Test AUC\", \"Test Accuracy\", \"Val AUC\", \"Val Accuracy\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "elif task_type.lower() == \"multiclass\":\n",
    "    for cand in [\"Test Accuracy\", \"Val Accuracy\", \"Test F1\", \"Val F1\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "else:  # regression\n",
    "    for cand in [\"Test RMSE\", \"Val RMSE\", \"Test R2\", \"Val R2\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "\n",
    "if pref_key:\n",
    "    print(f\"→ catboost: {pref_key} = {aggregates[pref_key]['mean']:.6f} ± {aggregates[pref_key]['std']:.6f}\")\n",
    "print(f\"Saved to: {out_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, categorical_cols, label_encoder, class_weight = preprocess_data(df, dataset_name=dataset_name, task_type=task_type, model_type=\"lightgbm\", seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "categorical_cols = config[\"categorical_cols\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=direction)\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    name=\"lightgbm\",\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    metric_name=metric_name,\n",
    "    SEED=SEED,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=class_weight,\n",
    "    categorical_cols=categorical_cols\n",
    "), n_trials=50)\n",
    "\n",
    "# Print best result summary\n",
    "best_trial = study.best_trial\n",
    "print(f\"\\nBest Trial: {best_trial.number}\")\n",
    "print(f\"  AUC Score: {best_trial.value:.4f}\")\n",
    "print(\"  Best Hyperparameters:\")\n",
    "for k, v in best_trial.params.items():\n",
    "    print(f\"    {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "\n",
    "# seeds & aggregation\n",
    "model_seeds = [0, 1, 2, 3, 4]\n",
    "numeric_keys = None\n",
    "per_seed_metrics = []\n",
    "\n",
    "# Where to save the summary file (the same folder you used before)\n",
    "summary_dir = os.path.join(save_dir, f\"lightgbm/best_model\")\n",
    "os.makedirs(summary_dir, exist_ok=True)\n",
    "out_file = os.path.join(summary_dir, \"best_results_mean.txt\")\n",
    "\n",
    "for s in model_seeds:\n",
    "    set_model_seed(s)  # your util to set np/torch/python seeds if needed\n",
    "    metrics = evaluate_best_model(\n",
    "        study,                 # Optuna study with .best_params\n",
    "        \"lightgbm\",                  # \"xgboost\" / \"lightgbm\" / \"catboost\" / etc.\n",
    "        task_type,\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        X_test, y_test,\n",
    "        categorical_cols=categorical_cols,\n",
    "        num_classes=num_classes,\n",
    "        SEED=s,                # <<< pass the seed into the model\n",
    "        device=device,\n",
    "        save_dir=save_dir,\n",
    "        class_weight=class_weight\n",
    "    )\n",
    "    if not isinstance(metrics, dict):\n",
    "        raise TypeError(f\"evaluate_best_model must return dict, got {type(metrics)}\")\n",
    "\n",
    "    if numeric_keys is None:\n",
    "        numeric_keys = [k for k, v in metrics.items() if isinstance(v, (Number, np.floating, np.integer))]\n",
    "    per_seed_metrics.append(metrics)\n",
    "\n",
    "    # brief print\n",
    "    brief = \", \".join(f\"{k}={float(metrics[k]):.6f}\" for k in numeric_keys[:6])\n",
    "    print(f\"Seed {s}: {brief}\")\n",
    "\n",
    "# Aggregate mean/std\n",
    "aggregates = {}\n",
    "for k in numeric_keys:\n",
    "    vals = [float(m[k]) for m in per_seed_metrics]\n",
    "    mean_k = float(np.mean(vals))\n",
    "    std_k  = float(np.std(vals, ddof=1)) if len(vals) > 1 else 0.0\n",
    "    aggregates[k] = {\"mean\": mean_k, \"std\": std_k}\n",
    "\n",
    "# Save YAML-like txt (same style as your example)\n",
    "with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Best trial re-evaluation across model seeds\\n\")\n",
    "    # If you really want to include patch_size (only exists for ViT/CNN), guard it:\n",
    "    if hasattr(study, \"best_params\") and \"patch_size\" in study.best_params:\n",
    "        f.write(f\"patch_size: {study.best_params['patch_size']}\\n\")\n",
    "    f.write(f\"seeds: {model_seeds}\\n\")\n",
    "    f.write(\"per_seed_metrics:\\n\")\n",
    "    for s, m in zip(model_seeds, per_seed_metrics):\n",
    "        f.write(f\"  - seed: {s}\\n\")\n",
    "        for k in numeric_keys:\n",
    "            f.write(f\"      {k}: {float(m[k]):.6f}\\n\")\n",
    "    f.write(\"aggregates:\\n\")\n",
    "    for k, mm in aggregates.items():\n",
    "        f.write(f\"  {k}:\\n\")\n",
    "        f.write(f\"    mean: {mm['mean']:.6f}\\n\")\n",
    "        f.write(f\"    std: {mm['std']:.6f}\\n\")\n",
    "\n",
    "# Console summary (pick a sensible key)\n",
    "pref_key = None\n",
    "if task_type.lower() == \"binary\":\n",
    "    for cand in [\"Test AUC\", \"Test Accuracy\", \"Val AUC\", \"Val Accuracy\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "elif task_type.lower() == \"multiclass\":\n",
    "    for cand in [\"Test Accuracy\", \"Val Accuracy\", \"Test F1\", \"Val F1\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "else:  # regression\n",
    "    for cand in [\"Test RMSE\", \"Val RMSE\", \"Test R2\", \"Val R2\"]:\n",
    "        if cand in aggregates: pref_key = cand; break\n",
    "\n",
    "if pref_key:\n",
    "    print(f\"→ lightgbm: {pref_key} = {aggregates[pref_key]['mean']:.6f} ± {aggregates[pref_key]['std']:.6f}\")\n",
    "print(f\"Saved to: {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
