{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "\n",
    "# Local application/library imports\n",
    "from utils import load_search_space\n",
    "\n",
    "import optuna\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    RocCurveDisplay, PrecisionRecallDisplay,\n",
    "    ConfusionMatrixDisplay, roc_auc_score, average_precision_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 64\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "# adult_income_cleaned, framingham_cleaned, preprocessed_heloc, diabetes\n",
    "dataset_name = 'boston'        \n",
    "dataset_subpath = 'Regression/boston'       \n",
    "task_type = 'Regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"./data/{dataset_subpath}/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LOAD AND PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target_tensor(y, task):\n",
    "    task = task.lower()\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.to_numpy()\n",
    "    elif isinstance(y, list):\n",
    "        y = np.array(y)\n",
    "        \n",
    "    if task == \"regression\" or task == \"binary\":\n",
    "        return torch.as_tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "    elif task == \"multiclass\":\n",
    "        return torch.as_tensor(y, dtype=torch.long)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(df, dataset_name, task_type, seed=42, batch_size=32, device='cpu'):\n",
    "    task_type = task_type.lower()\n",
    "\n",
    "    # Load config\n",
    "    with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    categorical_cols = config[\"categorical_cols\"]\n",
    "    numerical_cols = config[\"numerical_cols\"]\n",
    "    encoding = config[\"encoding\"]\n",
    "\n",
    "    # Extract features and target\n",
    "    X = df[numerical_cols + categorical_cols].copy()\n",
    "    y = df.iloc[:, -1].copy()\n",
    "\n",
    "    # Encode target if needed\n",
    "    le = None\n",
    "    if encoding.get(\"target\") == \"label\":\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "        label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    else:\n",
    "        label_mapping = None\n",
    "\n",
    "    # Split raw data before transformation\n",
    "    if task_type == \"regression\":\n",
    "        # For regression, we can use a simple split\n",
    "        X_train_raw, X_temp_raw, y_train, y_temp = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=seed\n",
    "        )\n",
    "        X_val_raw, X_test_raw, y_val, y_test = train_test_split(\n",
    "            X_temp_raw, y_temp, test_size=0.5, random_state=seed\n",
    "        )\n",
    "    else:\n",
    "        # For classification, we need stratified splits\n",
    "        X_train_raw, X_temp_raw, y_train, y_temp = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=seed, stratify=y\n",
    "        )\n",
    "        X_val_raw, X_test_raw, y_val, y_test = train_test_split(\n",
    "            X_temp_raw, y_temp, test_size=0.5, random_state=seed, stratify=y_temp\n",
    "        )\n",
    "\n",
    "    # Compute class weights for classification\n",
    "    class_weight = None\n",
    "    if task_type in [\"binary\", \"multiclass\"]:\n",
    "        # Compute raw weights\n",
    "        class_weight_values = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "        classes_sorted = np.sort(np.unique(y_train))\n",
    "        \n",
    "        if task_type == \"binary\":\n",
    "            # Compute pos_weight = weight for class 1 / weight for class 0\n",
    "            weight_dict = dict(zip(classes_sorted, class_weight_values))\n",
    "            pos_weight = weight_dict[1] / weight_dict[0]\n",
    "            class_weight = torch.tensor(pos_weight, dtype=torch.float32).to(device)\n",
    "            print(f\"Binary pos_weight (for BCEWithLogitsLoss): {class_weight.item()}\")\n",
    "\n",
    "        elif task_type == \"multiclass\":\n",
    "            class_weight = torch.tensor(class_weight_values, dtype=torch.float32).to(device)\n",
    "            print(f\"Multiclass class weights (for CrossEntropyLoss): {class_weight.tolist()}\")\n",
    "\n",
    "    # Transform numerical and categorical features\n",
    "    transformers = []\n",
    "\n",
    "    if encoding[\"numerical_features\"] == \"minmax\":\n",
    "        transformers.append((\"num\", MinMaxScaler(), numerical_cols))\n",
    "    elif encoding[\"numerical_features\"] == \"standard\":\n",
    "        transformers.append((\"num\", StandardScaler(), numerical_cols))\n",
    "\n",
    "    if categorical_cols and encoding[\"categorical_features\"] == \"onehot\":\n",
    "        transformers.append((\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), categorical_cols))\n",
    "\n",
    "    if transformers:\n",
    "        preprocessor = ColumnTransformer(transformers=transformers)\n",
    "        X_train = preprocessor.fit_transform(X_train_raw)\n",
    "        X_val = preprocessor.transform(X_val_raw)\n",
    "        X_test = preprocessor.transform(X_test_raw)\n",
    "\n",
    "        # Recover transformed column names\n",
    "        if \"cat\" in preprocessor.named_transformers_:\n",
    "            cat_feature_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_cols)\n",
    "            all_feature_names = numerical_cols + list(cat_feature_names)\n",
    "        else:\n",
    "            all_feature_names = numerical_cols + categorical_cols\n",
    "\n",
    "        X_train_num = pd.DataFrame(X_train, columns=all_feature_names, index=X_train_raw.index)\n",
    "        X_val_num = pd.DataFrame(X_val, columns=all_feature_names, index=X_val_raw.index)\n",
    "        X_test_num = pd.DataFrame(X_test, columns=all_feature_names, index=X_test_raw.index)\n",
    "    else:\n",
    "        all_feature_names = numerical_cols + categorical_cols  # or keep original order\n",
    "        X_train_num = pd.DataFrame(X_train_raw, columns=all_feature_names, index=X_train_raw.index)\n",
    "        X_val_num = pd.DataFrame(X_val_raw, columns=all_feature_names, index=X_val_raw.index)\n",
    "        X_test_num = pd.DataFrame(X_test_raw, columns=all_feature_names, index=X_test_raw.index)\n",
    "\n",
    "\n",
    "    print(f\"Shapes — Train: {X_train_num.shape}, Val: {X_val_num.shape}, Test: {X_test_num.shape}\")\n",
    "    print(f\"Numerical features: {len(numerical_cols)} — {numerical_cols}\")\n",
    "    print(f\"Categorical features: {len(categorical_cols)} — {categorical_cols}\")\n",
    "    print(f\"Total features: {X_train_num.shape[1]}\")\n",
    "    if label_mapping:\n",
    "        print(f\"Target label mapping: {label_mapping}\")\n",
    "    \n",
    "\n",
    "    attributes = len(X_train_num.columns)\n",
    "\n",
    "    print(\"Attributes: \", attributes)\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_num_tensor = torch.as_tensor(X_train_num.values, dtype=torch.float32)\n",
    "    X_val_num_tensor = torch.as_tensor(X_val_num.values, dtype=torch.float32)\n",
    "    X_test_num_tensor = torch.as_tensor(X_test_num.values, dtype=torch.float32)\n",
    "    y_train_tensor = prepare_target_tensor(y_train, task_type)\n",
    "    y_val_tensor = prepare_target_tensor(y_val, task_type)\n",
    "    y_test_tensor = prepare_target_tensor(y_test, task_type)\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    #X_train_img_tensor = X_train_img_tensor / 255.0\n",
    "    #X_val_img_tensor = X_val_img_tensor / 255.0\n",
    "    #X_test_img_tensor = X_test_img_tensor / 255.0\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset( X_train_num_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_num_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_num_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, attributes,  le, class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MODEL ARCHITECTURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, attributes, params, task, num_classes=None):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        # MLP branch\n",
    "        mlp_layers = []\n",
    "        input_dim = attributes\n",
    "        for hidden_dim in params[\"mlp_hidden_dims\"]:\n",
    "            mlp_layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            mlp_layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        # Determine output layer\n",
    "        output_dim = 1 if task in ['regression', 'binary'] else num_classes\n",
    "        mlp_layers.append(nn.Linear(input_dim, output_dim))\n",
    "        self.mlp = nn.Sequential(*mlp_layers) \n",
    "\n",
    "        # Change identity to something else if needed\n",
    "        self.activation = nn.Identity()\n",
    "\n",
    "    def forward(self, num_input):\n",
    "        x = self.mlp(num_input)\n",
    "        return self.activation(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## COMPILE AND FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import copy\n",
    "\n",
    "from models.utils import get_loss_fn, calculate_metrics, calculate_metrics_from_numpy, get_class_weighted_loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "def compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, \n",
    "                    model_name, trial_name=None, task='regression', epochs=200, max_lr=1, \n",
    "                    div_factor=10, final_div_factor=1, device='cuda', weight_decay=1e-2, save_model=False, class_weights=None, save_dir=None, study=None, verbose=False):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if class_weights != None:\n",
    "        loss_fn = get_class_weighted_loss_fn(task, class_weights)\n",
    "    else:\n",
    "        loss_fn = get_loss_fn(task)\n",
    "\n",
    "    # Compute min_lr from max_lr and div_factor\n",
    "    min_lr = max_lr / div_factor\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=min_lr, weight_decay=weight_decay)\n",
    "    \n",
    "    total_steps = epochs * len(train_loader)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, div_factor=div_factor, final_div_factor=final_div_factor, total_steps=total_steps, pct_start=0.3, anneal_strategy=\"cos\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    best_epoch = 0\n",
    "    #early_stopping_counter = 0\n",
    "    #patience = 10  # Early stopping patience\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'learning_rate': [], 'epoch_time': []}\n",
    "\n",
    "    if task == 'regression':\n",
    "        history.update({'train_mse': [],  'val_mse': [], 'train_mae': [],  'val_mae': [], 'train_rmse': [], 'val_rmse': [], 'train_r2': [], 'val_r2': []})\n",
    "    elif task in ['binary', 'multiclass']:\n",
    "        history.update({'train_accuracy': [], 'val_accuracy': [], 'train_precision': [], 'val_precision': [], 'train_recall': [], 'val_recall': [], 'train_f1': [], 'val_f1': []})\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "\n",
    "        for num_data, targets in train_loader:\n",
    "            num_data, targets = num_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(num_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(outputs.cpu().detach().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        if task == 'multiclass':\n",
    "            y_train_pred = np.vstack(train_preds)\n",
    "            y_train_true = train_targets\n",
    "        else:\n",
    "            y_train_pred = np.concatenate(train_preds)\n",
    "            y_train_true = np.concatenate(train_targets)\n",
    "            \n",
    "        train_metrics = calculate_metrics_from_numpy(y_train_true, y_train_pred, task)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for num_data, targets in val_loader:\n",
    "                num_data, targets = num_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "                outputs = model(num_data)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        if task == 'multiclass':\n",
    "            y_val_pred = np.vstack(val_preds)\n",
    "            y_val_true = val_targets\n",
    "        else:\n",
    "            y_val_pred = np.concatenate(val_preds)\n",
    "            y_val_true = np.concatenate(val_targets)\n",
    "        \n",
    "        val_metrics = calculate_metrics_from_numpy(y_val_true, y_val_pred, task)\n",
    "        \n",
    "        # Get the current learning rate\n",
    "        current_lr = scheduler.get_last_lr()\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "\n",
    "        for k, v in train_metrics.items():\n",
    "            history[f'train_{k}'].append(v)\n",
    "        for k, v in val_metrics.items():\n",
    "            history[f'val_{k}'].append(v)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch + 1\n",
    "            #early_stopping_counter = 0\n",
    "        #else:\n",
    "        #    early_stopping_counter += 1\n",
    "        #    if early_stopping_counter >= patience:\n",
    "        #        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        #        break\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # Recompute metrics using the best model\n",
    "    train_metrics, y_true_train, y_pred_train, y_prob_train = calculate_metrics(model, train_loader, device, class_weights, task)\n",
    "    val_metrics, y_true_val, y_pred_val, y_prob_val  = calculate_metrics(model, val_loader, device, class_weights, task)\n",
    "    test_metrics, y_true_test, y_pred_test, y_prob_test = calculate_metrics(model, test_loader, device, class_weights, task)\n",
    "\n",
    "    # Store recomputed metrics\n",
    "    metrics = {\n",
    "        'train_loss': train_metrics['loss'],\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'min_lr': min_lr,\n",
    "        'max_lr': max_lr,\n",
    "        'total_time': total_time,\n",
    "        'average_epoch_time': sum(history['epoch_time']) / len(history['epoch_time'])\n",
    "    }\n",
    "\n",
    "    # Add task-specific metrics\n",
    "    for k in train_metrics:\n",
    "        if k != 'loss':\n",
    "            metrics[f'train_{k}'] = train_metrics[k]\n",
    "    for k in val_metrics:\n",
    "        if k != 'loss':\n",
    "            metrics[f'val_{k}'] = val_metrics[k]\n",
    "    for k in test_metrics:\n",
    "        if k != 'loss':\n",
    "            metrics[f'test_{k}'] = test_metrics[k]\n",
    "    \n",
    "    if verbose:   \n",
    "        print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "        print(f\"Best model found at epoch {best_epoch}/{epochs}\")\n",
    "        print(f\"Best Train Loss: {metrics['train_loss']:.4f}, Best Val Loss: {metrics['val_loss']:.4f}\")\n",
    "        print(metrics)\n",
    "    \n",
    "    if save_model:\n",
    "        save_path = os.path.join(save_dir, f\"{model_name}/best_model/{trial_name}\")\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        plot_metric(history['train_loss'], history['val_loss'], 'Loss', save_path)\n",
    "        if task == 'regression':\n",
    "            plot_metric(history['train_mse'], history['val_mse'], 'MSE', save_path)\n",
    "            plot_metric(history['train_rmse'], history['val_rmse'], 'RMSE', save_path)\n",
    "        else:\n",
    "            plot_metric(history['train_accuracy'], history['val_accuracy'], 'Accuracy', save_path)\n",
    "            plot_metric(history['train_f1'], history['val_f1'], 'F1', save_path)\n",
    "\n",
    "        plot_learning_rate(history['learning_rate'], save_path)\n",
    "\n",
    "        # Save metrics\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        with open(f'{save_path}/best_model_metrics.txt', 'w') as f:\n",
    "            for key, value in metrics.items():\n",
    "                f.write(f'{key}: {value}\\n')\n",
    "\n",
    "        # Save model\n",
    "        torch.save(best_model, f\"{save_path}/best_model.pth\")\n",
    "        print(f\"Best model saved to {save_path}/best_model.pth\")\n",
    "\n",
    "        # Additional plots for classification\n",
    "        if task in [\"binary\"]:\n",
    "            plot_extra(\"Train\", y_true_train, y_pred_train, y_prob_train, save_path)\n",
    "            plot_extra(\"Validation\", y_true_val, y_pred_val, y_prob_val, save_path)\n",
    "            plot_extra(\"Test\", y_true_test, y_pred_test, y_prob_test, save_path)\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_extra(split_name, y_true, y_pred, y_prob, save_path):\n",
    "    y_true = y_true.ravel()\n",
    "    y_pred = y_pred.ravel()\n",
    "\n",
    "    # ROC Curve\n",
    "    RocCurveDisplay.from_predictions(y_true, y_prob)\n",
    "    auc_score = roc_auc_score(y_true, y_prob)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')\n",
    "    plt.title(f\"{split_name} ROC Curve (AUC = {auc_score:.2f})\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_path, f\"{split_name.lower()}_roc_curve.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    PrecisionRecallDisplay.from_predictions(y_true, y_prob)\n",
    "    avg_prec = average_precision_score(y_true, y_prob)\n",
    "    plt.title(f\"{split_name} PR Curve (AP = {avg_prec:.2f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_path, f\"{split_name.lower()}_pr_curve.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    # Normalized confusion matrix\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize='true').plot(cmap='Blues')\n",
    "    plt.title(f\"{split_name} Confusion Matrix (Normalized)\")\n",
    "    plt.grid(False)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.savefig(os.path.join(save_path, f\"{split_name.lower()}_confusion_matrix_normalized.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    # Raw confusion matrix\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize=None).plot(cmap='Blues')\n",
    "    plt.title(f\"{split_name} Confusion Matrix (Counts)\")\n",
    "    plt.grid(False)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.savefig(os.path.join(save_path, f\"{split_name.lower()}_confusion_matrix_counts.png\"))\n",
    "    plt.close(\"all\")\n",
    "\n",
    "\n",
    "def plot_metric(train_metric, val_metric, metric_name, save_path):\n",
    "    plt.figure()\n",
    "    plt.plot(train_metric, label=f'Train {metric_name}')\n",
    "    plt.plot(val_metric, label=f'Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.title(f'{metric_name} vs. Epoch')\n",
    "    save_path = f\"{save_path}/{metric_name.lower()}_plot.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close(\"all\")\n",
    "\n",
    "def plot_learning_rate(learning_rates, save_path):\n",
    "    plt.figure()\n",
    "    plt.plot(learning_rates)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate vs. Epoch')\n",
    "    save_path = f\"{save_path}/learning_rate_plot.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir =  os.path.join(\"logs\", task_type, dataset_name)\n",
    "model_name = \"mlp\"\n",
    "\n",
    "# Load config\n",
    "with open(f\"./configs/preprocess/{dataset_name}.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "batch_size = config[\"batch_size\"]\n",
    "epochs = [100,200]\n",
    "n_trials = 50\n",
    "\n",
    "if task_type.lower() == 'multiclass':\n",
    "    num_classes = df.iloc[:,-1].nunique()\n",
    "else:\n",
    "    num_classes = 1\n",
    "\n",
    "device='cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, model_name, task_type, \n",
    "              train_loader, val_loader, test_loader,\n",
    "              attributes, num_classes=None,\n",
    "              device='cuda', save_dir=None, class_weight=None, epochs=100):\n",
    "    \n",
    "    task = task_type.lower()\n",
    "    \n",
    "    params = load_search_space(model_name, trial)\n",
    "\n",
    "    params[\"mlp_hidden_dims\"] = json.loads(params[\"mlp_hidden_dims\"])\n",
    "    \n",
    "    with open(f\"configs/optuna_search/{model_name}.json\", \"r\") as f:\n",
    "        full_config = json.load(f)\n",
    "\n",
    "    config = full_config[model_name][\"fit\"]  # Access the model key\n",
    "\n",
    "    # Build and train model\n",
    "    model = MLP(attributes, params, task, num_classes)\n",
    "    metrics = compile_and_fit(\n",
    "        model,\n",
    "        train_loader, val_loader, test_loader,\n",
    "        dataset_name=dataset_name,\n",
    "        model_name=f\"trial_{trial.number}\",\n",
    "        task=task,  # assumed to be defined externally\n",
    "        max_lr=trial.suggest_float(\"max_lr\", config[\"max_lr\"][1], config[\"max_lr\"][2], log=True),\n",
    "        div_factor=trial.suggest_int(\"div_factor\", config[\"div_factor\"][1], config[\"div_factor\"][2]),\n",
    "        final_div_factor=trial.suggest_int(\"final_div_factor\", config[\"final_div_factor\"][1], config[\"final_div_factor\"][2]),\n",
    "        weight_decay=trial.suggest_float(\"weight_decay\", config[\"weight_decay\"][1], config[\"weight_decay\"][2], log=True),\n",
    "        epochs=trial.suggest_categorical(\"epochs\", [100, 200]),\n",
    "        save_model=False,\n",
    "        class_weights=class_weight\n",
    "    )\n",
    "\n",
    "    save_dir = os.path.join(save_dir, model_name, \"optuna\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if task == 'regression':\n",
    "        score = metrics[\"val_rmse\"]\n",
    "        with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "            f.write(f\"Trial {trial.number} - VAL-RMSE: {score:.4f}, Params: {params}\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    elif task == 'binary':\n",
    "        score = metrics[\"val_roc_auc\"]\n",
    "        with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "            f.write(f\"Trial {trial.number} - VAL-AUC: {score:.4f}, Params: {params}\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    elif task == 'multiclass':\n",
    "        score = metrics[\"val_accuracy\"]\n",
    "        with open(f\"{save_dir}/optuna_trials_log.txt\", \"a\") as f:\n",
    "            f.write(f\"Trial {trial.number} - VAL-Accuracy: {score:.4f}, Params: {params}\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task_type}\")\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_best_model(best_trial, train_loader, val_loader, test_loader, \n",
    "                        dataset_name, task_type, save_dir, attributes, trial_name,\n",
    "                        class_weight=None, num_classes=None, epochs=10):\n",
    "\n",
    "    task = task_type.lower()\n",
    "    best_params = best_trial.params\n",
    "\n",
    "    print(f\"\\nBest Trial: {best_trial.number}\")\n",
    "    print(f\"  Best Score: {best_trial.value:.4f}\")\n",
    "    print(\"  Best Hyperparameters:\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"    {k}: {v}\")\n",
    "\n",
    "    # Extract architecture-related parameters\n",
    "    architecture_params = {\n",
    "        k: v for k, v in best_params.items()\n",
    "        if k in [\"mlp_hidden_dims\"]\n",
    "    }\n",
    "\n",
    "    # Convert JSON string to list if necessary\n",
    "    if isinstance(architecture_params.get(\"mlp_hidden_dims\"), str):\n",
    "        architecture_params[\"mlp_hidden_dims\"] = json.loads(architecture_params[\"mlp_hidden_dims\"])\n",
    "\n",
    "    # Initialize model\n",
    "    model = MLP(attributes, architecture_params, task, num_classes)\n",
    "\n",
    "    # Train and evaluate\n",
    "    metrics = compile_and_fit(\n",
    "        model,\n",
    "        train_loader, val_loader, test_loader,\n",
    "        dataset_name=dataset_name,\n",
    "        model_name=model_name,\n",
    "        trial_name=f\"trial_{best_trial.number}\",\n",
    "        task=task,\n",
    "        max_lr=best_params[\"max_lr\"],\n",
    "        div_factor=best_params[\"div_factor\"],\n",
    "        final_div_factor=best_params[\"final_div_factor\"],\n",
    "        weight_decay=best_params[\"weight_decay\"],\n",
    "        epochs=best_params[\"epochs\"],\n",
    "        save_model=True,\n",
    "        class_weights=class_weight,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "\n",
    "    # Save best hyperparameters\n",
    "    params_file = os.path.join(save_dir, f\"{model_name}/best_model/{trial_name}\", \"best_params.json\")\n",
    "    os.makedirs(os.path.dirname(params_file), exist_ok=True)\n",
    "\n",
    "    with open(params_file, \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_model_seed(seed: int):\n",
    "    # Python built-in RNG\n",
    "    random.seed(seed)\n",
    "    # NumPy RNG\n",
    "    np.random.seed(seed)\n",
    "    # Torch RNG\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you use multi-GPU\n",
    "    \n",
    "    # For reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, label_encoder, class_weight  = load_and_preprocess_data(df, dataset_name, task_type, seed=SEED, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\" if task_type.lower() == \"regression\" else \"maximize\")\n",
    "study.optimize(lambda trial: objective(\n",
    "    trial=trial,\n",
    "    model_name=model_name,\n",
    "    task_type=task_type,\n",
    "    num_classes=num_classes ,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    attributes=attributes,\n",
    "    device=device,\n",
    "    save_dir=save_dir,\n",
    "    class_weight=class_weight,\n",
    "    epochs=epochs\n",
    "), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "\n",
    "# --- Configure which seeds to use for stability reporting ---\n",
    "model_seeds = [0, 1, 2, 3, 4]   # change as needed\n",
    "numeric_keys = None  # we’ll infer from first run\n",
    "\n",
    "# Determine study direction safely (minimize by default if unknown)\n",
    "def is_minimize_study(study):\n",
    "    try:\n",
    "        return study.direction == optuna.study.StudyDirection.MINIMIZE\n",
    "    except Exception:\n",
    "        try:\n",
    "            return study.directions[0] == optuna.study.StudyDirection.MINIMIZE\n",
    "        except Exception:\n",
    "            return True  # fallback\n",
    "\n",
    "minimize = is_minimize_study(study)\n",
    "\n",
    "# --- Pick the single best completed trial across ALL patch sizes ---\n",
    "completed = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "if not completed:\n",
    "    raise RuntimeError(\"No completed trials in the study.\")\n",
    "\n",
    "best_trial = (min if minimize else max)(completed, key=lambda t: t.value)\n",
    "\n",
    "\n",
    "trial_name = f\"trial_{best_trial.number}\"\n",
    "print(f\"\\nEvaluating overall best trial \"\n",
    "      f\"(Trial {best_trial.number}, ValObjective: {best_trial.value:.4f}\")\n",
    "    \n",
    "save_path = os.path.join(save_dir, f\"{model_name}/best_model/{trial_name}\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "per_seed_metrics = []\n",
    "\n",
    "for s in model_seeds:\n",
    "    set_model_seed(s)\n",
    "    metrics = evaluate_best_model(\n",
    "        best_trial,\n",
    "        train_loader, val_loader, test_loader,\n",
    "        dataset_name=dataset_name,\n",
    "        task_type=task_type,\n",
    "        save_dir=save_dir,\n",
    "        attributes=attributes,\n",
    "        class_weight=class_weight,\n",
    "        num_classes=num_classes,\n",
    "        epochs=epochs,\n",
    "        trial_name=trial_name\n",
    "        # If evaluate_best_model accepts a seed arg, pass model_seed=s\n",
    "    )\n",
    "    if not isinstance(metrics, dict):\n",
    "        raise TypeError(f\"evaluate_best_model must return dict, got: {type(metrics)}\")\n",
    "\n",
    "    # infer numeric keys once (ints, floats, numpy scalars)\n",
    "    if numeric_keys is None:\n",
    "        numeric_keys = [k for k, v in metrics.items()\n",
    "                        if isinstance(v, (Number, np.floating, np.integer))]\n",
    "    per_seed_metrics.append(metrics)\n",
    "\n",
    "    # brief per-seed printout\n",
    "    log_bits = []\n",
    "    for k in [\"test_loss\", \"val_loss\", \"train_loss\"]:\n",
    "        if k in metrics and isinstance(metrics[k], (Number, np.floating, np.integer)):\n",
    "            log_bits.append(f\"{k}={float(metrics[k]):.6f}\")\n",
    "    print(f\"  Seed {s}: \" + (\", \".join(log_bits) if log_bits else str(metrics)))\n",
    "\n",
    "# Aggregate mean/std per numeric key\n",
    "aggregates = {}\n",
    "for k in numeric_keys:\n",
    "    vals = [float(m[k]) for m in per_seed_metrics]\n",
    "    mean_k = float(np.mean(vals))\n",
    "    std_k = float(np.std(vals, ddof=1)) if len(vals) > 1 else 0.0\n",
    "    aggregates[k] = {\"mean\": mean_k, \"std\": std_k}\n",
    "\n",
    "# Save YAML-like txt\n",
    "out_file = os.path.join(save_path, \"best_results_mean.txt\")\n",
    "with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Overall best trial re-evaluation across model seeds\\n\")\n",
    "    f.write(f\"trial_number: {best_trial.number}\\n\")\n",
    "    if model_name == \"ViT_with_register_tokens\":\n",
    "        f.write(f\"patch_size: {best_patch}\\n\")\n",
    "    f.write(f\"val_objective_best: {best_trial.value:.6f}\\n\")\n",
    "    f.write(f\"direction: {'minimize' if minimize else 'maximize'}\\n\")\n",
    "    f.write(f\"seeds: {model_seeds}\\n\")\n",
    "    f.write(\"per_seed_metrics:\\n\")\n",
    "    for s, m in zip(model_seeds, per_seed_metrics):\n",
    "        f.write(f\"  - seed: {s}\\n\")\n",
    "        for k in numeric_keys:\n",
    "            f.write(f\"      {k}: {float(m[k]):.6f}\\n\")\n",
    "    f.write(\"aggregates:\\n\")\n",
    "    for k, mm in aggregates.items():\n",
    "        f.write(f\"  {k}:\\n\")\n",
    "        f.write(f\"    mean: {mm['mean']:.6f}\\n\")\n",
    "        f.write(f\"    std: {mm['std']:.6f}\\n\")\n",
    "\n",
    "# Console summary\n",
    "if \"test_loss\" in aggregates:\n",
    "    print(\"  → test_loss Mean ± Std: \"\n",
    "          f\"{aggregates['test_loss']['mean']:.6f} ± {aggregates['test_loss']['std']:.6f}\")\n",
    "elif \"val_loss\" in aggregates:\n",
    "    print(\"  → val_loss Mean ± Std: \"\n",
    "          f\"{aggregates['val_loss']['mean']:.6f} ± {aggregates['val_loss']['std']:.6f}\")\n",
    "\n",
    "print(f\"Saved to: {out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
